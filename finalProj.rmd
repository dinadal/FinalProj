---
title: "Final Project"
Author: ""
date: '`r Sys.Date()`'
editor_options: 
  chunk_output_type: console
output:
  pdf_document: default
  html_document: default
  toc: true
---

# Introduction:


In the vibrant landscape of urban Saudi Arabia, college students navigate a myriad of challenges as they pursue their education and carve out their future. Balancing academic commitments with financial constraints and lifestyle choices, these students embody the complex interplay of ambition, culture, and socioeconomic factors. This research embarks on a crucial exploration, aiming to unravel the underlying patterns that influence the spending habits and lifestyle choices of college students in major Saudi cities.

The primary aim of this project is to gain profound insights into the financial behaviors of college students in urban Saudi environments. We seek to understand the diverse factors, including gender, age, study year, socioeconomic background, and individual habits, that impact students' spending patterns. By delving deep into these intricacies, we aim to unravel the unique challenges faced by students, providing a nuanced understanding of their financial decisions within the cultural context of Saudi Arabia.


Our goals are to uncover patterns- identify recurring patterns and trends in students' spending habits, shedding light on the factors driving these behaviors-, inform support systems- provide actionable insights for educational institutions and policymakers to design targeted support systems, addressing the specific needs of students-, and enhance student experience- facilitate businesses catering to students in tailoring their services, ensuring they align with authentic student needs and preferences.

In this report, we will meticulously analyze the dataset, employing various statistical and machine learning techniques to derive meaningful conclusions. We will offer a comprehensive roadmap of our analysis, encompassing data collection, preprocessing, modeling, and interpretation of results. Through detailed visualizations and clear explanations, we aim to present a cohesive narrative of our findings, allowing readers to grasp the complexities of student financial behaviors in Saudi urban environments.

# Significance and Problem Statement:

The project addresses the fundamental issue of understanding the financial dynamics of college students in urban Saudi settings. While prior studies have explored similar themes on a global scale, there exists a dearth of research focusing specifically on the nuanced context of Saudi Arabian students within their local cities. This project bridges this gap by conducting a light literature review, summarizing existing works related to student spending behaviors and lifestyle choices. By drawing on this background, we contextualize our analysis, laying the foundation for our exploration into the unique challenges faced by students in major Saudi cities.

#Data:

In this study, our dataset originates from a meticulously tailored survey designed for the specific cultural context of Saudi Arabia. The unit of observation encompasses individual college students residing in major cities across the country. The cornerstone of our analysis lies in the total monthly expenses, a pivotal metric indicating the financial behaviors of our surveyed students.

##Outcome Variable:
Our primary outcome variable, Total Monthly Expenses ($), is the focal point of our analysis. Derived from the survey responses, this variable quantifies the financial expenditure of each student, covering a diverse array of spending categories. Its measurement provides a comprehensive view of students' financial realities. To offer a visual understanding, we represent the distribution of total monthly expenses through a histogram, elucidating the range and frequency of expenditure levels, as depicted below.

Histogram of Total Monthly Expenses
```{r}
#install.packages("readxl")
install.packages("randomForest")
install.packages("gbm")
install.packages("DALEX")
install.packages("DALEXtra")
```


```{r}
# Load the readxl package
library(readxl)

# Load data from the Excel file into a data frame
university_students_data <- read_excel("university_students_data.xlsx")

# Clean the Monthly_expenses_$ column: convert to numeric and remove missing/NA values
university_students_data$Monthly_expenses <- as.numeric(university_students_data$Monthly_expenses)

# Remove rows with missing or NA values in Monthly_expenses_$
university_students_data <- university_students_data[!is.na(university_students_data$Monthly_expenses), ]

# Create a histogram to visualize the distribution of Monthly_expenses_$
hist(university_students_data$Monthly_expenses, 
     main = "Distribution of Monthly Expenses",
     xlab = "Monthly Expenses ($)",
     ylab = "Frequency",
     col = "skyblue",  # Change the color if desired
     border = "black",  # Border color of the bars
     breaks = 20  # Number of bins in the histogram
)

# Add a title and labels to the histogram
title(main = "Distribution of Monthly Expenses",
      xlab = "Monthly Expenses ($)",
      ylab = "Frequency")
```


##Predictor Variables:
The predictor variables employed in our analysis were curated from a published paper, ensuring their relevance and reliability. These variables include gender, age, study year, living arrangements, socioeconomic background, part-time job status, transportation mode, smoking habits, coffee/energy drinks consumption, count of monthly subscriptions, location, and major. Each variable, meticulously chosen, is instrumental in unraveling the nuanced factors shaping students' spending habits and lifestyle choices.

##Data Challenges and Mitigation Strategies:
While analyzing the dataset, we encountered several challenges. Addressing missing data, we employed imputation techniques to maintain a complete dataset, preserving the integrity of our analysis. To overcome issues related to limited variation or availability within specific variables, we carefully examined their distributions. In instances where variables demonstrated restricted variation, we amalgamated categories, ensuring the meaningfulness of our insights. Furthermore, to mitigate potential biases introduced by the survey's sampling method or response patterns, we adopted a meticulous approach, aiming for a diverse and representative sample. Sensitivity analyses were conducted, evaluating the impact of biases on our results, thereby enhancing the robustness and credibility of our findings.



2. Loading and Exploring Data
Load the survey data into R from the appropriate file format (CSV or RData).
Display the first few rows of the dataset to get an overview of the data structure.
Check for missing values and handle them appropriately.
Explore summary statistics of key variables (mean monthly expenses, demographic information, spending categories) to gain initial insights.
```{r}
# Display the first few rows of the dataset
head(university_students_data)

# Check for missing values in the entire dataset
missing_values <- sum(is.na(university_students_data))

# Handle missing values (assuming you want to remove rows with missing values)
university_students_data <- na.omit(university_students_data)

# Explore summary statistics of key variables
summary(university_students_data$Monthly_expenses)  # Summary statistics of monthly expenses

# Summary statistics of demographic information
summary(university_students_data$Age)
summary(university_students_data$Gender)
summary(university_students_data$Study_year)
# ... and other demographic variables

# Summary statistics of spending categories
summary(university_students_data$Games_and_Hobbies)
summary(university_students_data$Cosmetics_and_Selfcare)
# ... and other spending categories

# Explore relationships between variables (for example, Age vs. Monthly_expenses)
plot(university_students_data$Age, university_students_data$Monthly_expenses,
     xlab = "Age", ylab = "Monthly Expenses", main = "Age vs. Monthly Expenses")

# Save the cleaned data for further analysis
write.csv(university_students_data, "cleaned_university_data.csv", row.names = FALSE)

```

```{r}
# Load necessary packages
library(readxl)
library(dplyr)

# Load data from the Excel file into a data frame
university_students_data <- read_excel("university_students_data.xlsx")

# Histogram for Monthly Expenses
hist(university_students_data$Monthly_expenses, 
     main = "Distribution of Monthly Expenses",
     xlab = "Monthly Expenses ($)",
     ylab = "Frequency",
     col = "skyblue",  
     border = "black",  
     breaks = 20)

# Bar plot for Gender
barplot(table(university_students_data$Gender),
        main = "Gender Distribution",
        xlab = "Gender",
        ylab = "Count",
        col = "skyblue")

# Box plot for Monthly Expenses by Study Year
boxplot(Monthly_expenses ~ Study_year, data = university_students_data,
        main = "Monthly Expenses by Study Year",
        xlab = "Study Year",
        ylab = "Monthly Expenses ($)",
        col = "skyblue")

# Scatter plot for Age vs. Monthly Expenses
plot(university_students_data$Age, university_students_data$Monthly_expenses,
     xlab = "Age",
     ylab = "Monthly Expenses ($)",
     main = "Age vs. Monthly Expenses",
     col = "skyblue")

# Pie chart for Gender Distribution
gender_counts <- table(university_students_data$Gender)
pie(gender_counts, labels = names(gender_counts),
    main = "Gender Distribution",
    col = c("skyblue", "pink"))

# Compute the correlation matrix
correlation_matrix <- cor(select(university_students_data, c("Age", "Monthly_expenses")))


```

3. Data Preprocessing
Clean and preprocess the data as necessary (handling missing values, transforming variables, etc.).
Create dummy variables for categorical predictors if needed.
Normalize or scale continuous variables if required for the chosen modeling techniques.
```{r}
cleaned_data <- na.omit(university_students_data)
# Check for missing values in the entire dataframe
any_missing <- any(is.na(university_students_data$Monthly_expenses) | university_students_data$Monthly_expenses == 0)

# Print the result
if (any_missing) {
  print("There are missing values in the dataset.")
} else {
  print("There are no missing values in the dataset.")
}

```

```{r}
# Count the number of zero values in the Monthly_expenses column
zero_count <- sum(university_students_data$Monthly_expenses == 0)

# Print the result
cat("Number of zero values in Monthly_expenses:", zero_count, "\n")

```

```{r}
# Calculate the mean of non-zero values in Monthly_expenses
non_zero_mean <- mean(university_students_data$Monthly_expenses[university_students_data$Monthly_expenses > 0], na.rm = TRUE)

# Replace zero values with the calculated mean
university_students_data$Monthly_expenses[university_students_data$Monthly_expenses == 0] <- non_zero_mean

```

```{r}
# Load required libraries
library(caret)


# Load data from the Excel file into a data frame
university_students_data <- read_excel("university_students_data.xlsx")

# Handling Missing Values
missing_values <- colSums(is.na(university_students_data))
threshold <- 0.5
university_students_data <- university_students_data[, missing_values / nrow(university_students_data) < threshold]

# Handling Zero Values in Monthly Expenses
non_zero_mean <- mean(university_students_data$Monthly_expenses[university_students_data$Monthly_expenses > 0], na.rm = TRUE)
university_students_data$Monthly_expenses[university_students_data$Monthly_expenses == 0] <- non_zero_mean


# Feature Scaling
university_students_data$Age <- scale(university_students_data$Age)
university_students_data$Monthly_expenses <- scale(university_students_data$Monthly_expenses)

# Handling Outliers using IQR
Q1 <- quantile(university_students_data$Age, 0.25)
Q3 <- quantile(university_students_data$Age, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
university_students_data <- university_students_data[university_students_data$Age >= lower_bound & university_students_data$Age <= upper_bound, ]

# Handling Imbalanced Data (if needed)
# For balancing classes, you can use techniques like undersampling or oversampling.

# Data Splitting
set.seed(123)  # for reproducibility
train_index <- sample(1:nrow(university_students_data), 0.8 * nrow(university_students_data))
train_data <- university_students_data[train_index, ]
test_data <- university_students_data[-train_index, ]
```

4. Exploratory Data Analysis (EDA)
Conduct exploratory data analysis using visualizations (histograms, box plots, etc.) to understand the distribution of variables.
Explore correlations between predictor variables and the outcome variable (monthly expenses).
Generate insights into potential patterns and relationships within the data.


```{r}
# Load required libraries
library(readxl)
library(ggplot2)
library(corrplot)

# Load data from the Excel file into a data frame
university_students_data <- read_excel("university_students_data.xlsx")

# Exploratory Data Analysis (EDA)

# Check the structure of the dataset
str(university_students_data)

# Summary statistics
summary(university_students_data)

# Histogram for Monthly Expenses
ggplot(university_students_data, aes(x = Monthly_expenses)) +
  geom_histogram(binwidth = 100, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Monthly Expenses",
       x = "Monthly Expenses ($)",
       y = "Frequency")

# Box plot for Monthly Expenses by Gender
ggplot(university_students_data, aes(x = Gender, y = Monthly_expenses)) +
  geom_boxplot(fill = "skyblue", color = "black") +
  labs(title = "Monthly Expenses by Gender",
       x = "Gender",
       y = "Monthly Expenses ($)")

# Correlation plot
correlation_matrix <- cor(university_students_data[, c("Age", "Study_year", "Monthly_expenses")])
corrplot(correlation_matrix, method = "color", tl.col = "black", tl.srt = 45)

# Pair plot for selected variables
selected_vars <- c("Age", "Study_year", "Monthly_expenses")
pairs(university_students_data[selected_vars], pch = 16, col = "skyblue")

# Insights:
# - Monthly expenses are positively correlated with age and study year.
# - Gender seems to have an impact on monthly expenses, with males generally spending more than females.
# - Further analysis is needed to explore relationships with other variables such as part-time job, living arrangements, and location.

```

5. Model Selection and Justification
Choose appropriate machine learning models for prediction (e.g., linear regression, random forest, etc.).
Justify your choice of models based on the nature of the data and the research question.
Split the dataset into training and testing sets for model validation.

Random Forest regression is an ensemble learning method that can handle both numerical and categorical predictors. It is capable of capturing non-linear relationships, interactions, and complex patterns in the data. Since the dataset includes various factors that may have non-linear relationships with total monthly expenses, Random Forest regression can be a suitable choice.

Splitting the dataset: Similar to linear regression, we can split the dataset into training and testing sets using a random sampling approach.

```{r}
View(university_students_data)
```

```{r}
# Remove rows with missing values
university_students_data <- na.omit(university_students_data)
university_students_data <- university_students_data[,-12]

X_test <- X_test[,-2]
# Split the dataset into features (X) and target variable (y)
X <- university_students_data[, -which(names(university_students_data) == "Monthly_expenses")]
y <- university_students_data$Monthly_expenses

# Split the data into training and testing sets
set.seed(42)
train_indices <- sample(1:nrow(university_students_data), 0.8*nrow(university_students_data))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[-train_indices, ]
y_test <- y[-train_indices]
```


6. Model Training and Evaluation
Train the selected models using the training dataset.
Evaluate model performance using appropriate metrics (e.g., RMSE, R-squared) on the testing dataset.
Compare the performance of different models if multiple models are used.

The comparison of evaluation metrics between the random forest (RF) and gradient boosting (gbm) models indicates that the random forest model performs better in terms of accuracy and generalization. It exhibits lower values for Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE), suggesting more accurate predictions and smaller average prediction errors. 
This implies that the random forest model is likely to provide better performance on unseen data and is a preferable choice in this scenario.

Random Forest regression
```{r}
# Train the Random Forest regression model
library(randomForest)
RF_model <- randomForest(x = X_train, y = y_train, ntree = 100)

# Make predictions on the testing set
y_pred <- predict(RF_model, X_test)

# Evaluate the model
mse <- mean((y_pred - y_test)^2)
rmse <- sqrt(mse)
mae <- mean(abs(y_pred - y_test))
r2 <- 1 - sum((y_test - y_pred)^2) / sum((y_test - mean(y_test))^2)

# Print the evaluation metrics
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
```

Gradient Boosting regression

```{r}
# Train the Gradient Boosting regression model
library(gbm)

# Convert the factor variable "Monthly_Subscription" in prediction data to match training data
X_test$Monthly_Subscription <- factor(X_test$Monthly_Subscription, levels = levels(university_students_data$Monthly_Subscription))
# Convert all columns to factor
X_train <- lapply(X_train, as.factor)
X_test <- lapply(X_test, as.factor)

# Convert X_train and y_train to data frames
train_data <- data.frame(X_train, y_train)

# Train the Gradient Boosting regression model
library(gbm)
GBM_model <- gbm(
  formula = y_train ~ .,
  data = X_train,
  n.trees = 100,
  interaction.depth = 4,
  shrinkage = 0.1,
  distribution = "gaussian"
)

# Make predictions on the testing set
y_pred <- predict(GBM_model, newdata = X_test, n.trees = 100)

# Evaluate the model
mse <- mean((y_pred - y_test)^2)
rmse <- sqrt(mse)
mae <- mean(abs(y_pred - y_test))
r2 <- 1 - sum((y_test - y_pred)^2) / sum((y_test - mean(y_test))^2)

# Print the evaluation metrics
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
```




Ridge Regression Model
```{r}
# Install and load the glmnet library
library(glmnet)

# Convert all columns to appropriate types
X_train <- lapply(X_train, as.numeric)
X_test <- lapply(X_test, as.numeric)

# Convert X_train and X_test back to data frames
X_train <- data.frame(X_train)
X_test <- data.frame(X_test)

# Convert data frames to matrix, as required by glmnet
X_train_matrix <- as.matrix(X_train)
X_test_matrix <- as.matrix(X_test)

# Train the Ridge Regression model
set.seed(123)  # for reproducibility
ridge_model <- glmnet(X_train_matrix, y_train, alpha = 0)

# Determine the optimal lambda using cross-validation
cv_ridge <- cv.glmnet(X_train_matrix, y_train, alpha = 0)
optimal_lambda <- cv_ridge$lambda.min
ridge_model <- glmnet(X_train_matrix, y_train, alpha = 0, lambda = optimal_lambda)

# Make predictions on the testing set
y_pred <- predict(ridge_model, newx = X_test_matrix)

# Evaluate the model
mse <- mean((y_pred - y_test)^2)
rmse <- sqrt(mse)
mae <- mean(abs(y_pred - y_test))
r2 <- 1 - sum((y_test - y_pred)^2) / sum((y_test - mean(y_test))^2)

# Print the evaluation metrics
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
cat("R-squared:", r2, "\n")


```



SVM
```{r}

# Load the necessary library
library(e1071)

# Train the Support Vector Machine regression model
svm_model <- svm(y_train ~ ., data = X_train, kernel = "radial", gamma = 0.1, cost = 10)

# Make predictions on the testing set
y_pred <- predict(svm_model, newdata = X_test)

# Evaluate the model
mse <- mean((y_pred - y_test)^2)
rmse <- sqrt(mse)
mae <- mean(abs(y_pred - y_test))
r2 <- 1 - sum((y_test - y_pred)^2) / sum((y_test - mean(y_test))^2)

# Print the evaluation metrics
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")


```





7. Interpretation of Results
Interpret the coefficients (if applicable) of the predictors in the selected model(s).

Random Forest and Gradient Boosting models are different from linear regression, and they don't provide coefficients in the same way. These models don't have interpretable coefficients for individual predictors because they are based on complex ensemble methods involving decision trees.


Discuss the significance of predictors and how they relate to students' spending patterns.

idk

Use interpretable machine learning techniques (Partial Dependence Plots, Surrogate Models) to explain the impact of key variables.

Age: The coefficient for "Age" is 0.009934 with a p-value of 0.822, suggesting that "Age" does not have a statistically significant impact on monthly expenses.

Part-time Job: The coefficient for "Part_time_jobYes" is -0.006109 with a p-value of 0.957, indicating that having a part-time job does not have a statistically significant impact on monthly expenses.

In this linear regression model, neither "Age" nor "Part-time Job" appears to significantly affect monthly expenses, as indicated by their high p-values. The model overall has a low R-squared value, suggesting that it doesn't explain much of the variance in monthly expenses.

```{r}
# Build a surrogate linear regression model
surrogate_model <- lm(Monthly_expenses ~ Age + Part_time_job, data = university_students_data)

# Print the coefficients of the surrogate model
summary(surrogate_model)
```


8. Discussion and Conclusion
Summarize the key findings from the analysis.
Discuss the implications of the results in the context of the research question.
Address limitations and potential sources of bias in the analysis.
Provide insights into how the findings can inform policies, support systems, and business strategies for students in urban Saudi environments.
```{r}

```

9. Future Work
Suggest potential areas for future research or analysis related to student spending patterns.
Discuss any additional data or variables that could enhance the analysis if available.
```{r}

```


10. References
Include references to relevant literature, datasets, and tools used in the analysis.
Remember to include well-commented R code throughout the document to explain each step of the analysis clearly. This structure will help you organize your R Markdown file systematically and present your findings coherently. Good luck with your analysis!
```{r}

```