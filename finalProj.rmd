---
title: "Final Project"
Author: ""
date: '`r Sys.Date()`'
editor_options: 
  chunk_output_type: console
output:
  pdf_document: default
  html_document: default
  toc: true
---


Histogram of Total Monthly Expenses


```{r}
# Load the readxl package
library(readxl)

# Load data from the Excel file into a data frame
university_students_data <- read_excel("university_students_data.xlsx")
```


```{r}
# Clean the Monthly_expenses_$ column: convert to numeric and remove missing/NA values
university_students_data$Monthly_expenses <- as.numeric(university_students_data$Monthly_expenses)

# Remove rows with missing or NA values in Monthly_expenses_$
cleaned_dataset <- na.omit(university_students_data)
```


```{r}

View(cleaned_dataset)

# Check for missing values in the entire dataset
any_missing <- any(is.na(cleaned_dataset))

# Output whether there are missing values or not
if (any_missing) {
  print("There are missing values in the dataset.")
} else {
  print("No missing values found in the dataset.")
}


```


```{r}
# Calculate the range of expenses
min_expense <- min(cleaned_dataset$Monthly_expenses)
max_expense <- max(cleaned_dataset$Monthly_expenses)

# Create a histogram with adjusted axes
hist(cleaned_dataset$Monthly_expenses, 
     main = "Distribution of Monthly Expenses",
     xlab = "Monthly Expenses (SAR)",
     ylab = "Frequency",
     col = "#69b3a2",
     border = "black",
     breaks = 20,
     xlim = c(min_expense, max_expense),  # Set x-axis limits
     ylim = c(0, max(table(cut(cleaned_dataset$Monthly_expenses, breaks = 20))) + 5)  # Set y-axis limits
)

# Add a title and labels to the histogram
title(main = "Distribution of Monthly Expenses",
      xlab = "Monthly Expenses (SAR)",
      ylab = "Frequency")




```






2. Loading and Exploring Data

```{r}
# Display the first few rows of the dataset
head(cleaned_dataset)

# Check for missing values in the entire dataset
missing_values <- sum(is.na(cleaned_dataset))
missing_values

```


```{r}
# Explore summary statistics of key variables
summary(cleaned_dataset$Monthly_expenses)  # Summary statistics of monthly expenses
```

```{r}
# Summary statistics of demographic information
summary(cleaned_dataset$Age)

```

```{r}
# Convert 'Gender' to factor
cleaned_dataset$Gender <- factor(cleaned_dataset$Gender)

# Summary table for Gender
gender_summary <- table(cleaned_dataset$Gender)
gender_summary

```

```{r}
# Convert 'Study_year' to factor
cleaned_dataset$Study_year <- factor(cleaned_dataset$Study_year)

# Summary table for Study_year
study_year_summary <- table(cleaned_dataset$Study_year)
study_year_summary


```


```{r}
# Summary of 'Games_and_Hobbies' 'Cosmetics_and_Selfcare'
table(cleaned_dataset$Games_and_Hobbies)
table(cleaned_dataset$Cosmetics_and_Selfcare)
table(cleaned_dataset$Smoking)


```

```{r}
# Frequency tables for all columns (including both numerical and categorical) (This gives a summary of everything in the dataset)
lapply(cleaned_dataset, table)


```


The dataset collected from college students in urban Saudi Arabia sheds light on various facets of their lifestyle and expenditure patterns. Analyzing key parameters reveals intriguing insights into their spending habits, lifestyle choices, and demographic distribution.

Demographic Overview:

Gender Distribution: The dataset portrays a relatively balanced gender representation, with 366 female and 331 male respondents. This balance suggests a relatively equal participation of both genders in the survey.

Age Range and Academic Year: The majority of respondents fall within the 18 to 23 age bracket, with significant representation from 18-year-olds (196 respondents) and 22-year-olds (210 respondents). 2nd-year students (290 respondents) dominate the academic year distribution.

Living Arrangements and Socioeconomic Background:

Living Arrangements: The dataset reflects a mix of living arrangements, with 341 respondents residing at home and 340 in dormitories, suggesting the diversity in living preferences among urban college students.

Socioeconomic Background: The participants come from varying socioeconomic backgrounds, with 249 respondents identifying with a low socioeconomic status, followed by 219 from a high and 229 from a medium socioeconomic background. This diversity might impact their spending behaviors and financial decisions.

Financial Behaviors and Expenditure:

Scholarship and Employment: A significant portion of respondents (206) reported having a scholarship, while 134 indicated having part-time jobs. This suggests a blend of financial aid and self-sustenance among urban college students.

Monthly Expenses: The data unveils a spectrum of monthly expenses, with the most common range falling between 300 and 1500 dollars. However, it's noteworthy that there are outliers reporting higher expenses, indicating potential variations in spending capacities.

Subscription Preferences: More respondents (410) indicated spending on monthly subscriptions compared to other categories like cosmetics and self-care (362) or games and hobbies (422), signifying an inclination towards certain lifestyle choices.

Interest Areas and Majors:

Academic Majors: The dataset represents a diverse range of academic majors, including Engineering, Medicine, Business, Computer Science, Art, and Others. This diversity in majors might influence spending habits based on the specific requirements of each field.



```{r}

library(ggplot2)

library(ggplot2)

# Calculate the mean or median expenses for each study year
expenses_by_year <- aggregate(cleaned_dataset$Monthly_expenses, 
                              by = list(Study_year = cleaned_dataset$Study_year), 
                              FUN = mean)  # Change to median if preferred

# Bar plot for mean/median expenses per study year
ggplot(expenses_by_year, aes(x = factor(Study_year), y = x)) +
  geom_bar(stat = "identity", fill = "#2c3e50") +
  labs(x = "Study Year", y = "Mean Monthly Expenses", 
       title = "Mean Monthly Expenses by Study Year") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, hjust = 0.5, margin = margin(b = 10)),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )



```

```{r}
# Bar chart of monthly expenses by location
ggplot(data = cleaned_dataset, aes(x = Location, y = Monthly_expenses, fill = Location)) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +
  labs(title = "Average Monthly Expenses by Location",
       x = "Location", y = "Average Monthly Expenses")


```


```{r}
library(dplyr)
```


```{r}
# Bar plot for Gender
barplot(table(cleaned_dataset$Gender),
        main = "Gender Distribution",
        xlab = "Gender",
        ylab = "Count",
        col = "skyblue")
```




```{r}
library(ggplot2)

# Scatter plot for Age vs. Monthly Expenses
plot(cleaned_dataset$Age, cleaned_dataset$Monthly_expenses,
     xlab = "Age",
     ylab = "Monthly Expenses ($)",
     main = "Age vs. Monthly Expenses",
     col = "black")


```


```{r}
# Pie chart for Gender Distribution
gender_counts <- table(cleaned_dataset$Gender)
pie(gender_counts, labels = names(gender_counts),
    main = "Gender Distribution",
    col = c("skyblue", "pink"))
```


```{r}
# Compute the correlation matrix
# Load the dplyr package
library(dplyr)

# Compute the correlation matrix
correlation_matrix <- cor(select(cleaned_dataset, c("Age", "Monthly_expenses")))
correlation_matrix



```

^ A correlation coefficient close to 0 suggests a very weak linear relationship between 'Age' and 'Monthly_expenses'. In this case, the correlation between these two variables is quite low, indicating a very weak linear association between a person's age and their monthly expenses in your dataset.


```{r}
# Assuming Gender is coded as numeric (0 and 1)
cleaned_dataset$Gender_numeric <- as.numeric(cleaned_dataset$Gender) - 1

# Compute the correlation matrix between 'Gender' and 'Monthly_expenses'
correlation_matrix <- cor(cleaned_dataset$Gender_numeric, cleaned_dataset$Monthly_expenses)
correlation_matrix

```

^The correlation coefficient you've obtained (approximately 0.0671) between 'Gender' (represented numerically) and 'Monthly_expenses' suggests a very weak positive linear relationship between these variables.

```{r}
# Convert 'Scholarship' to numeric (if it's a categorical variable)
cleaned_dataset$Scholarship_numeric <- as.numeric(cleaned_dataset$Scholarship == "Yes")

# Compute the correlation between 'Scholarship' and 'Monthly_expenses'
correlation_matrix <- cor(cleaned_dataset$Scholarship_numeric, cleaned_dataset$Monthly_expenses)
correlation_matrix

```

The correlation coefficient of approximately 0.0012 between 'Scholarship' (represented numerically) and 'Monthly_expenses' suggests an extremely weak positive linear relationship between these variables.


```{r}

# Convert 'Location' into dummy variables
dummy_location <- model.matrix(~ cleaned_dataset$Location - 1)  # -1 removes intercept

# Combine Monthly_expenses and dummy_location
data_with_dummies <- cbind(cleaned_dataset["Monthly_expenses"], dummy_location)

# Compute correlation
correlation_matrix <- cor(data_with_dummies)
correlation_matrix["Monthly_expenses", -1]  # Exclude Monthly_expenses row

```

Jeddah shows a slight positive relationship, suggesting a small tendency for higher monthly expenses among individuals in that location.
Khobar, Madinah, and Makkah all exhibit negative correlations, indicating a tendency for lower monthly expenses in these areas.
Riyadh displays a stronger positive correlation, implying a stronger tendency for higher monthly expenses compared to the other locations


```{r}
# Convert 'Part_time_job' to a numeric variable
cleaned_dataset$Part_time_job_numeric <- ifelse(cleaned_dataset$Part_time_job == "Yes", 1, 0)

# Calculate correlation between Part_time_job_numeric and Monthly_expenses
cor(cleaned_dataset$Part_time_job_numeric, cleaned_dataset$Monthly_expenses)



```

A correlation coefficient of approximately 0.001 suggests a very weak or negligible linear relationship between having a part-time job ('Part_time_job') and monthly expenses ('Monthly_expenses'). This value close to zero indicates that there's almost no linear association between these two variables in your dataset.


```{r}
# Convert 'Coffee_or_Energy_Drinks' to a numeric variable
cleaned_dataset$Coffee_numeric <- ifelse(cleaned_dataset$Coffee_or_Energy_Drinks == "Yes", 1, 0)

# Calculate correlation between Coffee_numeric and Monthly_expenses
cor(cleaned_dataset$Coffee_numeric, cleaned_dataset$Monthly_expenses)


```

A correlation coefficient of approximately 0.0497 suggests a very weak or negligible linear relationship between consuming coffee or energy drinks ('Coffee_or_Energy_Drinks') and monthly expenses ('Monthly_expenses'). This value close to zero indicates that there's almost no linear association between these two variables in your dataset.

```{r}
# Filter out non-numeric columns
numeric_cols <- cleaned_dataset[sapply(cleaned_dataset, is.numeric)]

# Calculate correlations with Monthly_expenses for numeric columns
correlation_with_expenses <- sapply(numeric_cols, function(x) cor(x, cleaned_dataset$Monthly_expenses))

# Sort correlations
correlation_with_expenses <- sort(correlation_with_expenses, decreasing = TRUE)
correlation_with_expenses


```

'Monthly_expenses' has a correlation of 1.0 with itself, which is expected.
'Gender_numeric' has a very weak positive correlation (0.067) with 'Monthly_expenses'.
'Coffee_numeric' also shows a very weak positive correlation (0.0497) with 'Monthly_expenses'.
'Age' has an extremely weak positive correlation (0.0263) with 'Monthly_expenses'.
'Scholarship_numeric' and 'Part_time_job_numeric' have negligible correlations (close to 0) with 'Monthly_expenses'.



```{r}
# Assuming 'Gender' is a factor, conduct Chi-squared test
chisq.test(cleaned_dataset$Gender, cleaned_dataset$Monthly_expenses)


```





3. Data Preprocessing
Clean and preprocess the data as necessary (handling missing values, transforming variables, etc.).
Create dummy variables for categorical predictors if needed.
Normalize or scale continuous variables if required for the chosen modeling techniques.

```{r}
# Count the number of zero values in the Monthly_expenses column
zero_count <- sum(cleaned_dataset$Monthly_expenses == 0)

# Print the result
cat("Number of zero values in Monthly_expenses:", zero_count, "\n")

```

```{r}
# Calculate the mean of non-zero values in Monthly_expenses
non_zero_mean <- mean(cleaned_dataset$Monthly_expenses[cleaned_dataset$Monthly_expenses > 0], na.rm = TRUE)

# Replace zero values with the calculated mean
cleaned_dataset$Monthly_expenses[cleaned_dataset$Monthly_expenses == 0] <- non_zero_mean

```

```{r}
# Load required libraries
library(caret)

# Copy the original data to a new variable
processed_data <- cleaned_dataset

# Handling Missing Values
missing_values <- colSums(is.na(processed_data))
threshold <- 0.5
processed_data <- processed_data[, missing_values / nrow(processed_data) < threshold]

# Handling Zero Values in Monthly Expenses
non_zero_mean <- mean(processed_data$Monthly_expenses[processed_data$Monthly_expenses > 0], na.rm = TRUE)
processed_data$Monthly_expenses[processed_data$Monthly_expenses == 0] <- non_zero_mean

# Feature Scaling
processed_data$Age <- scale(processed_data$Age)
processed_data$Monthly_expenses <- scale(processed_data$Monthly_expenses)

# Handling Outliers using IQR
Q1 <- quantile(processed_data$Age, 0.25)
Q3 <- quantile(processed_data$Age, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
processed_data <- processed_data[processed_data$Age >= lower_bound & processed_data$Age <= upper_bound, ]
# Handling Imbalanced Data (if needed)
# For balancing classes, you can use techniques like undersampling or oversampling.

# Data Splitting
set.seed(123)  # for reproducibility
train_index <- sample(1:nrow(processed_data), 0.8 * nrow(processed_data))
train_data <- processed_data[train_index, ]
test_data <- processed_data[-train_index, ]
```

4. Exploratory Data Analysis (EDA)
Conduct exploratory data analysis using visualizations (histograms, box plots, etc.) to understand the distribution of variables.
Explore correlations between predictor variables and the outcome variable (monthly expenses).
Generate insights into potential patterns and relationships within the data.


```{r}
# Load required libraries
library(readxl)
library(ggplot2)
library(corrplot)

# Load data from the Excel file into a data frame
university_students_data <- read_excel("university_students_data.xlsx")

# Exploratory Data Analysis (EDA)

# Check the structure of the dataset
str(university_students_data)

# Summary statistics
summary(university_students_data)

# Histogram for Monthly Expenses
ggplot(university_students_data, aes(x = Monthly_expenses)) +
  geom_histogram(binwidth = 100, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Monthly Expenses",
       x = "Monthly Expenses ($)",
       y = "Frequency")

# Box plot for Monthly Expenses by Gender
ggplot(university_students_data, aes(x = Gender, y = Monthly_expenses)) +
  geom_boxplot(fill = "skyblue", color = "black") +
  labs(title = "Monthly Expenses by Gender",
       x = "Gender",
       y = "Monthly Expenses ($)")

# Correlation plot
correlation_matrix <- cor(university_students_data[, c("Age", "Study_year", "Monthly_expenses")])
corrplot(correlation_matrix, method = "color", tl.col = "black", tl.srt = 45)

# Pair plot for selected variables
selected_vars <- c("Age", "Study_year", "Monthly_expenses")
pairs(university_students_data[selected_vars], pch = 16, col = "skyblue")

# Insights:
# - Monthly expenses are positively correlated with age and study year.
# - Gender seems to have an impact on monthly expenses, with males generally spending more than females.
# - Further analysis is needed to explore relationships with other variables such as part-time job, living arrangements, and location.

```

5. Model Selection and Justification
Choose appropriate machine learning models for prediction (e.g., linear regression, random forest, etc.).
Justify your choice of models based on the nature of the data and the research question.
Split the dataset into training and testing sets for model validation.

Random Forest regression is an ensemble learning method that can handle both numerical and categorical predictors. It is capable of capturing non-linear relationships, interactions, and complex patterns in the data. Since the dataset includes various factors that may have non-linear relationships with total monthly expenses, Random Forest regression can be a suitable choice.

Splitting the dataset: Similar to linear regression, we can split the dataset into training and testing sets using a random sampling approach.

```{r}
View(university_students_data)
```

```{r}
# Remove rows with missing values
processed_data <- na.omit(processed_data)

# Split the dataset into features (X) and target variable (y)
X <- processed_data[, -which(names(processed_data) == "Monthly_expenses")]
y <- processed_data$Monthly_expenses

# Split the data into training and testing sets
set.seed(42)
train_indices <- sample(1:nrow(processed_data), 0.8*nrow(processed_data))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[-train_indices, ]
y_test <- y[-train_indices]
```






6. Model Training and Evaluation

Random Forest regression

```{r}
# Train the Random Forest regression model
library(randomForest)
RF_model <- randomForest(x = X_train, y = y_train, ntree = 100)

# Make predictions on the testing set
y_pred <- predict(RF_model, X_test)

# Evaluate the model
mse <- mean((y_pred - y_test)^2)
rmse <- sqrt(mse)
mae <- mean(abs(y_pred - y_test))
r2 <- 1 - sum((y_test - y_pred)^2) / sum((y_test - mean(y_test))^2)

# Print the evaluation metrics
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
```


 Linear Regression model


```{r}
# Train the Linear Regression model
lm_model <- lm(y_train ~ ., data = X_train)

# Make predictions on the testing set
y_pred <- predict(lm_model, newdata = X_test)

# Evaluate the model
mse <- mean((y_pred - y_test)^2)
rmse <- sqrt(mse)
mae <- mean(abs(y_pred - y_test))
r2 <- 1 - sum((y_test - y_pred)^2) / sum((y_test - mean(y_test))^2)

# Print the evaluation metrics
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
```

SVM
```{r}
# Load the necessary package
library(e1071)

# Train the SVM model
svm_model <- svm(y_train ~ ., data = X_train)

# Make predictions on the testing set
y_pred <- predict(svm_model, newdata = X_test)

# Evaluate the model
mse <- mean((y_pred - y_test)^2, na.rm = TRUE) # Adding na.rm = TRUE to remove NA values if they exist
rmse <- sqrt(mse)
mae <- mean(abs(y_pred - y_test), na.rm = TRUE) # Adding na.rm = TRUE to remove NA values if they exist
r2 <- 1 - sum((y_test - y_pred)^2, na.rm = TRUE) / sum((y_test - mean(y_test, na.rm = TRUE))^2, na.rm = TRUE) # Adding na.rm = TRUE to remove NA values if they exist

# Print the evaluation metrics
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
cat("R squared:", r2, "\n")
```

Gradient Boosting regression

The choice of employing Gradient Boosting Regression for predicting monthly expenses among college students in urban Saudi Arabia was driven by several factors:

Enhanced Predictive Power: Gradient Boosting Regression is known for its ability to build powerful predictive models by iteratively improving weak learners, minimizing errors, and producing strong ensemble models.

Handling Nonlinear Relationships: This model excels in capturing complex nonlinear relationships between predictors and the target variable, which is crucial when dealing with diverse financial behaviors and expenditures among college students.

Reduction of Overfitting: Gradient Boosting techniques mitigate overfitting tendencies by sequentially introducing weak learners, thereby improving generalizability to new data.

```{r}
# Train the Gradient Boosting regression model
library(gbm)

# Convert the factor variable "Monthly_Subscription" in prediction data to match training data
X_test$Monthly_Subscription <- factor(X_test$Monthly_Subscription, levels = levels(university_students_data$Monthly_Subscription))
# Convert all columns to factor
X_train <- lapply(X_train, as.factor)
X_test <- lapply(X_test, as.factor)

# Convert X_train and y_train to data frames
train_data <- data.frame(X_train, y_train)

# Train the Gradient Boosting regression model
library(gbm)
GBM_model <- gbm(
  formula = y_train ~ .,
  data = X_train,
  n.trees = 100,
  interaction.depth = 4,
  shrinkage = 0.1,
  distribution = "gaussian"
)

# Make predictions on the testing set
y_pred <- predict(GBM_model, newdata = X_test, n.trees = 100)

# Evaluate the model
mse <- mean((y_pred - y_test)^2)
rmse <- sqrt(mse)
mae <- mean(abs(y_pred - y_test))
r2 <- 1 - sum((y_test - y_pred)^2) / sum((y_test - mean(y_test))^2)

# Print the evaluation metrics
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
```






```{r}
# Create a dataframe for the model metrics
models <- c("Random Forest", "Linear Regression", "SVM", "Gradient Boosting")
MSE <- c(0.3299, 0.2885, 0.3058, 0.2334)
RMSE <- c(0.5744, 0.5371, 0.5530, 0.4831)
MAE <- c(0.4121, 0.3583, 0.3645, 0.3295)

df <- data.frame(models, MSE, RMSE, MAE)

# Plotting the bar graph
library(ggplot2)
library(dplyr)
library(tidyr)

df_long <- df %>%
  pivot_longer(cols = -models, names_to = "Metric", values_to = "Value")

# Define colors for the fill
my_colors <- c("MSE" = "#efb04d", "RMSE" = "#5242f8", "MAE" = "#7af842")

ggplot(df_long, aes(x = models, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  labs(title = "Comparison of Model Performance Metrics",
       x = "Models",
       y = "Metric Values") +
  scale_fill_manual(values = my_colors) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.title = element_blank(),
        legend.position = "top") +
  guides(fill = guide_legend(title = "Metrics"))


```




7. Interpretation of Results





8. Discussion and Conclusion




9. Future Work



```{r}

```


10. References
Include references to relevant literature, datasets, and tools used in the analysis.
Remember to include well-commented R code throughout the document to explain each step of the analysis clearly. This structure will help you organize your R Markdown file systematically and present your findings coherently. Good luck with your analysis!
```{r}

```